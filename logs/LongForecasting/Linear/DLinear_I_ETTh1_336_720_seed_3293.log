Args in experiment:
Namespace(activation='gelu', batch_size=32, c_out=7, checkpoints='./checkpoints/', d_ff=2048, d_layers=1, d_model=512, data='ETTh1', data_path='ETTh1.csv', dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, individual=True, is_training=1, itr=1, label_len=48, learning_rate=0.005, loss='mse', lradj='type1', model='DLinear', model_id='ETTh1_336_720', moving_avg=25, n_heads=8, num_workers=0, output_attention=False, patience=5, pred_len=720, root_path='./dataset/', save_pred_values=False, seed=3293, seq_len=336, target='OT', test_flop=False, train_epochs=20, train_only=False, use_amp=False, use_gpu=False, use_multi_gpu=False)
Use CPU
>>>>>>>start training : ETTh1_336_720_DLinear_ETTh1_ftM_sl336_ll48_pl720_dm512_nh8_el2_dl1_df2048_fc1_ebtimeF_dtTrue_Exp_0_seed3293>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7585
val 2161
test 2161
	iters: 100, epoch: 1 | loss: 0.6992244
	speed: 0.0348s/iter; left time: 161.4249s
	iters: 200, epoch: 1 | loss: 0.6185907
	speed: 0.0322s/iter; left time: 146.3729s
Epoch: 1 cost time: 8.185314893722534
Epoch: 1, Steps: 237 | Train Loss: 0.6328765 Vali Loss: 1.4906634 Test Loss: 0.6546512
Validation loss decreased (inf --> 1.490663).  Saving model ...
Updating learning rate to 0.005
	iters: 100, epoch: 2 | loss: 0.6118318
	speed: 0.0650s/iter; left time: 286.0969s
	iters: 200, epoch: 2 | loss: 0.6826242
	speed: 0.0363s/iter; left time: 156.1566s
Epoch: 2 cost time: 8.589272499084473
Epoch: 2, Steps: 237 | Train Loss: 0.6067571 Vali Loss: 1.4882423 Test Loss: 0.5862849
Validation loss decreased (1.490663 --> 1.488242).  Saving model ...
Updating learning rate to 0.0025
	iters: 100, epoch: 3 | loss: 0.5694345
	speed: 0.0620s/iter; left time: 258.3311s
	iters: 200, epoch: 3 | loss: 0.5082092
	speed: 0.0348s/iter; left time: 141.5736s
Epoch: 3 cost time: 8.32217025756836
Epoch: 3, Steps: 237 | Train Loss: 0.5364795 Vali Loss: 1.3502133 Test Loss: 0.5296138
Validation loss decreased (1.488242 --> 1.350213).  Saving model ...
Updating learning rate to 0.00125
	iters: 100, epoch: 4 | loss: 0.4516877
	speed: 0.0636s/iter; left time: 250.0106s
	iters: 200, epoch: 4 | loss: 0.5039137
	speed: 0.0400s/iter; left time: 153.2516s
Epoch: 4 cost time: 8.836728811264038
Epoch: 4, Steps: 237 | Train Loss: 0.5074811 Vali Loss: 1.3167924 Test Loss: 0.4912289
Validation loss decreased (1.350213 --> 1.316792).  Saving model ...
Updating learning rate to 0.000625
	iters: 100, epoch: 5 | loss: 0.5317205
	speed: 0.0652s/iter; left time: 240.7292s
	iters: 200, epoch: 5 | loss: 0.4626091
	speed: 0.0341s/iter; left time: 122.5663s
Epoch: 5 cost time: 8.458775520324707
Epoch: 5, Steps: 237 | Train Loss: 0.4925809 Vali Loss: 1.2711332 Test Loss: 0.4985910
Validation loss decreased (1.316792 --> 1.271133).  Saving model ...
Updating learning rate to 0.0003125
	iters: 100, epoch: 6 | loss: 0.4399508
	speed: 0.0616s/iter; left time: 212.8795s
	iters: 200, epoch: 6 | loss: 0.5108035
	speed: 0.0382s/iter; left time: 128.0400s
Epoch: 6 cost time: 8.525708675384521
Epoch: 6, Steps: 237 | Train Loss: 0.4848212 Vali Loss: 1.2446915 Test Loss: 0.4905617
Validation loss decreased (1.271133 --> 1.244691).  Saving model ...
Updating learning rate to 0.00015625
	iters: 100, epoch: 7 | loss: 0.4777154
	speed: 0.0629s/iter; left time: 202.6057s
	iters: 200, epoch: 7 | loss: 0.4716438
	speed: 0.0385s/iter; left time: 120.1826s
Epoch: 7 cost time: 8.911770820617676
Epoch: 7, Steps: 237 | Train Loss: 0.4805448 Vali Loss: 1.2440352 Test Loss: 0.4878508
Validation loss decreased (1.244691 --> 1.244035).  Saving model ...
Updating learning rate to 7.8125e-05
	iters: 100, epoch: 8 | loss: 0.4478608
	speed: 0.0600s/iter; left time: 178.8254s
	iters: 200, epoch: 8 | loss: 0.4504400
	speed: 0.0364s/iter; left time: 104.8134s
Epoch: 8 cost time: 8.246379375457764
Epoch: 8, Steps: 237 | Train Loss: 0.4781653 Vali Loss: 1.2361600 Test Loss: 0.4927298
Validation loss decreased (1.244035 --> 1.236160).  Saving model ...
Updating learning rate to 3.90625e-05
	iters: 100, epoch: 9 | loss: 0.4984025
	speed: 0.0594s/iter; left time: 162.9302s
	iters: 200, epoch: 9 | loss: 0.4828460
	speed: 0.0368s/iter; left time: 97.2131s
Epoch: 9 cost time: 8.270219802856445
Epoch: 9, Steps: 237 | Train Loss: 0.4769746 Vali Loss: 1.2410749 Test Loss: 0.4862552
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.953125e-05
	iters: 100, epoch: 10 | loss: 0.4808501
	speed: 0.0590s/iter; left time: 148.0469s
	iters: 200, epoch: 10 | loss: 0.5114478
	speed: 0.0372s/iter; left time: 89.6072s
Epoch: 10 cost time: 8.408923864364624
Epoch: 10, Steps: 237 | Train Loss: 0.4762990 Vali Loss: 1.2424670 Test Loss: 0.4852949
EarlyStopping counter: 2 out of 5
Updating learning rate to 9.765625e-06
	iters: 100, epoch: 11 | loss: 0.4658761
	speed: 0.0642s/iter; left time: 145.8818s
	iters: 200, epoch: 11 | loss: 0.4336789
	speed: 0.0367s/iter; left time: 79.7203s
Epoch: 11 cost time: 8.534244775772095
Epoch: 11, Steps: 237 | Train Loss: 0.4759594 Vali Loss: 1.2415329 Test Loss: 0.4854870
EarlyStopping counter: 3 out of 5
Updating learning rate to 4.8828125e-06
	iters: 100, epoch: 12 | loss: 0.4203649
	speed: 0.0600s/iter; left time: 121.9750s
	iters: 200, epoch: 12 | loss: 0.4719581
	speed: 0.0378s/iter; left time: 73.0903s
Epoch: 12 cost time: 8.322722911834717
Epoch: 12, Steps: 237 | Train Loss: 0.4757127 Vali Loss: 1.2437884 Test Loss: 0.4864581
EarlyStopping counter: 4 out of 5
Updating learning rate to 2.44140625e-06
	iters: 100, epoch: 13 | loss: 0.4552719
	speed: 0.0626s/iter; left time: 112.4806s
	iters: 200, epoch: 13 | loss: 0.4405263
	speed: 0.0350s/iter; left time: 59.3381s
Epoch: 13 cost time: 8.11954140663147
Epoch: 13, Steps: 237 | Train Loss: 0.4756391 Vali Loss: 1.2434287 Test Loss: 0.4863178
EarlyStopping counter: 5 out of 5
Early stopping
>>>>>>>testing : ETTh1_336_720_DLinear_ETTh1_ftM_sl336_ll48_pl720_dm512_nh8_el2_dl1_df2048_fc1_ebtimeF_dtTrue_Exp_0_seed3293<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.49174824357032776, mae:0.4896879196166992
