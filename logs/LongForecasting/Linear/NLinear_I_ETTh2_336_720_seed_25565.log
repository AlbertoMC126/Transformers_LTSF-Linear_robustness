Args in experiment:
Namespace(activation='gelu', batch_size=32, c_out=7, checkpoints='./checkpoints/', d_ff=2048, d_layers=1, d_model=512, data='ETTh2', data_path='ETTh2.csv', dec_in=7, des='Exp', devices='0,1,2,3', distil=True, do_predict=False, dropout=0.05, e_layers=2, embed='timeF', embed_type=0, enc_in=7, factor=1, features='M', freq='h', gpu=0, individual=True, is_training=1, itr=1, label_len=48, learning_rate=0.005, loss='mse', lradj='type1', model='NLinear', model_id='ETTh2_336_720', moving_avg=25, n_heads=8, num_workers=0, output_attention=False, patience=5, pred_len=720, root_path='./dataset/', save_pred_values=False, seed=25565, seq_len=336, target='OT', test_flop=False, train_epochs=20, train_only=False, use_amp=False, use_gpu=False, use_multi_gpu=False)
Use CPU
>>>>>>>start training : ETTh2_336_720_NLinear_ETTh2_ftM_sl336_ll48_pl720_dm512_nh8_el2_dl1_df2048_fc1_ebtimeF_dtTrue_Exp_0_seed25565>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7585
val 2161
test 2161
Total number of trainable parameters: 1698480
Total number of parameters: 1698480
	iters: 100, epoch: 1 | loss: 1.0699805
	speed: 0.0411s/iter; left time: 190.6299s
	iters: 200, epoch: 1 | loss: 0.8978902
	speed: 0.0418s/iter; left time: 189.6014s
Epoch: 1 cost time: 9.700718879699707
Epoch: 1, Steps: 237 | Train Loss: 0.9129431 Vali Loss: 0.7473699 Test Loss: 0.5094945
Validation loss decreased (inf --> 0.747370).  Saving model ...
Updating learning rate to 0.005
	iters: 100, epoch: 2 | loss: 0.7359248
	speed: 0.0666s/iter; left time: 293.4540s
	iters: 200, epoch: 2 | loss: 0.6363680
	speed: 0.0416s/iter; left time: 178.9742s
Epoch: 2 cost time: 9.584185600280762
Epoch: 2, Steps: 237 | Train Loss: 0.8670871 Vali Loss: 0.7535625 Test Loss: 0.5303473
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.0025
	iters: 100, epoch: 3 | loss: 0.8325889
	speed: 0.0670s/iter; left time: 279.1252s
	iters: 200, epoch: 3 | loss: 0.9160215
	speed: 0.0400s/iter; left time: 162.6994s
Epoch: 3 cost time: 9.402651309967041
Epoch: 3, Steps: 237 | Train Loss: 0.8059114 Vali Loss: 0.6628286 Test Loss: 0.4451489
Validation loss decreased (0.747370 --> 0.662829).  Saving model ...
Updating learning rate to 0.00125
	iters: 100, epoch: 4 | loss: 0.6232653
	speed: 0.0672s/iter; left time: 263.9636s
	iters: 200, epoch: 4 | loss: 0.8320131
	speed: 0.0400s/iter; left time: 153.1355s
Epoch: 4 cost time: 9.542783498764038
Epoch: 4, Steps: 237 | Train Loss: 0.7664809 Vali Loss: 0.6340481 Test Loss: 0.4271174
Validation loss decreased (0.662829 --> 0.634048).  Saving model ...
Updating learning rate to 0.000625
	iters: 100, epoch: 5 | loss: 0.8394775
	speed: 0.0703s/iter; left time: 259.5815s
	iters: 200, epoch: 5 | loss: 1.0049938
	speed: 0.0395s/iter; left time: 141.8680s
Epoch: 5 cost time: 9.53482437133789
Epoch: 5, Steps: 237 | Train Loss: 0.7578331 Vali Loss: 0.6340084 Test Loss: 0.4243493
Validation loss decreased (0.634048 --> 0.634008).  Saving model ...
Updating learning rate to 0.0003125
	iters: 100, epoch: 6 | loss: 0.6897955
	speed: 0.0703s/iter; left time: 242.8582s
	iters: 200, epoch: 6 | loss: 0.7191213
	speed: 0.0403s/iter; left time: 135.3908s
Epoch: 6 cost time: 9.590347290039062
Epoch: 6, Steps: 237 | Train Loss: 0.7515837 Vali Loss: 0.6433235 Test Loss: 0.4189960
EarlyStopping counter: 1 out of 5
Updating learning rate to 0.00015625
	iters: 100, epoch: 7 | loss: 1.0430698
	speed: 0.0661s/iter; left time: 212.8253s
	iters: 200, epoch: 7 | loss: 0.8460549
	speed: 0.0397s/iter; left time: 123.6880s
Epoch: 7 cost time: 9.435384273529053
Epoch: 7, Steps: 237 | Train Loss: 0.7480540 Vali Loss: 0.6344119 Test Loss: 0.4195616
EarlyStopping counter: 2 out of 5
Updating learning rate to 7.8125e-05
	iters: 100, epoch: 8 | loss: 0.6113774
	speed: 0.0660s/iter; left time: 196.7322s
	iters: 200, epoch: 8 | loss: 0.8728955
	speed: 0.0399s/iter; left time: 114.8984s
Epoch: 8 cost time: 9.339388847351074
Epoch: 8, Steps: 237 | Train Loss: 0.7466589 Vali Loss: 0.6313716 Test Loss: 0.4210544
Validation loss decreased (0.634008 --> 0.631372).  Saving model ...
Updating learning rate to 3.90625e-05
	iters: 100, epoch: 9 | loss: 0.7609016
	speed: 0.0668s/iter; left time: 183.3236s
	iters: 200, epoch: 9 | loss: 0.8166087
	speed: 0.0415s/iter; left time: 109.7265s
Epoch: 9 cost time: 9.340588569641113
Epoch: 9, Steps: 237 | Train Loss: 0.7452795 Vali Loss: 0.6363750 Test Loss: 0.4186528
EarlyStopping counter: 1 out of 5
Updating learning rate to 1.953125e-05
	iters: 100, epoch: 10 | loss: 0.8039351
	speed: 0.0608s/iter; left time: 152.4205s
	iters: 200, epoch: 10 | loss: 0.7739362
	speed: 0.0319s/iter; left time: 76.7831s
Epoch: 10 cost time: 7.84235954284668
Epoch: 10, Steps: 237 | Train Loss: 0.7448962 Vali Loss: 0.6354871 Test Loss: 0.4194496
EarlyStopping counter: 2 out of 5
Updating learning rate to 9.765625e-06
	iters: 100, epoch: 11 | loss: 0.6237801
	speed: 0.0482s/iter; left time: 109.4406s
	iters: 200, epoch: 11 | loss: 0.6444404
	speed: 0.0272s/iter; left time: 58.9863s
Epoch: 11 cost time: 6.427353858947754
Epoch: 11, Steps: 237 | Train Loss: 0.7444393 Vali Loss: 0.6371408 Test Loss: 0.4193021
EarlyStopping counter: 3 out of 5
Updating learning rate to 4.8828125e-06
	iters: 100, epoch: 12 | loss: 0.5956251
	speed: 0.0393s/iter; left time: 79.8430s
	iters: 200, epoch: 12 | loss: 0.6565647
	speed: 0.0191s/iter; left time: 37.0208s
Epoch: 12 cost time: 4.849500894546509
Epoch: 12, Steps: 237 | Train Loss: 0.7442061 Vali Loss: 0.6364327 Test Loss: 0.4192363
EarlyStopping counter: 4 out of 5
Updating learning rate to 2.44140625e-06
	iters: 100, epoch: 13 | loss: 0.7688360
	speed: 0.0280s/iter; left time: 50.3133s
	iters: 200, epoch: 13 | loss: 0.8250029
	speed: 0.0116s/iter; left time: 19.7476s
Epoch: 13 cost time: 3.085552930831909
Epoch: 13, Steps: 237 | Train Loss: 0.7441423 Vali Loss: 0.6361702 Test Loss: 0.4192350
EarlyStopping counter: 5 out of 5
Early stopping
Total training time: 121.9213 seconds
>>>>>>>testing : ETTh2_336_720_NLinear_ETTh2_ftM_sl336_ll48_pl720_dm512_nh8_el2_dl1_df2048_fc1_ebtimeF_dtTrue_Exp_0_seed25565<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.4190141558647156, mae:0.4501468241214752
