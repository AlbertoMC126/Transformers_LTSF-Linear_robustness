# Unveiling the Limitations of Transformer Models in Time Series Forecasting

This repository reproduces and expands the LTSF-Linear code: "[Are Transformers Effective for Time Series Forecasting?](https://arxiv.org/pdf/2205.13504.pdf)" to study training robustness and stability of Transformers and LTSF-Linear models for time series forecasting. 

## Features
- We provide with a deterministic approach for the training to improve reproducibility that allows user to work directly with seeds in the Shell scripts. 
- CPU and GPU .yml files to use both implementation approaches. CPU allows for parallel training of LTSF-Linear that is much faster (dependant on the actual device) than gpu for all but electricity and traffic datasets. GPU allows for faster training of transformer-based models and LTSF-Linear with large datasets. 
- Dataset_exploration notebook provides some tools to inspect datasets for periodicities and trends in terms of ACF, PACF, and  Fourier transform, and also to test features for stationarity
- Results_farm provides an overview of performance results. The whole review is mostly automated based on logs files generated by training the models. Note logs/ contains all the files generated in our experiments for Linear and Transformers, so the notebook can be directly executed.


## Description
This code is simply built on the code base of LTSF-Linear. We appreciate the following GitHub repos a lot for their valuable code base or datasets:

The implementation of LTSF-Linear is from https://github.com/cure-lab/LTSF-Linear/tree/main

The implementation of Autoformer, Informer, and Transformer is from https://github.com/thuml/Autoformer

The implementation of FEDformer is from https://github.com/MAZiqing/FEDformer

We provide all experiment script files in `./scripts`.

## Getting Started
### Environment Requirements

First, please make sure you have installed Conda. Then, our environment can be installed by using either cpu_environment.yml or gpu_environment.yml. 
```
conda env create -f cpu_environment.yml
```
Our original setup was built on Windows 11 and the Anaconda distribution.

### Data Preparation

You can obtain all the dataset benchmarks from [Google Drive](https://drive.google.com/drive/folders/1ZOYpTUa82_jCcxIdTmyr0LXQfvaM9vIy), originally provided in Autoformer. All the datasets are preprocessed and can be easily used. They must be located in the `./dataset` directory for the scripts to function.

### Training Example
- In `scripts/ `, the training scripts for *LTSF-Linear/Autoformer/Informer/Transformer* are provided
- In `FEDformer/scripts/`, the training scripts for *FEDformer* are provided

For example:

To train the Autoformer, Informer, and Transformer on the nine datasets, you can use the script `scripts/Formers_final`:
```
sh scripts/Formers_final.sh
```
It will start to train all the models iteratively by default, the results will be shown in `logs/LongForecasting/Transformers`. You can specify the name of the model in the script.

All scripts to train Transformers and LTSF-Linear are in `scripts/`, you can run them in a similar way. The default look-back window for LTSF-Linear is 336, while for Transformers is 96. Linear_final_cpu.sh trains 10 linear models with different seeds in parallel in CPU. You can modify this by eliminating the **&** at the end of each  python -u run_longExp.py command. This execution makes training much faster in our Intel(R) Core(TM) i9-12900K and 64 RAM setup, but it might hinders the execution in a different one, so we recomend to train iteratively or in the gpu with Linear_final_gpu.sh, just comment/uncomment the experiments you want to make.

Scripts of FEDformer are in `FEDformer/scripts` and can be run in a similar way, just do first `cd FEDformer` to go to the directory. Logs will be stored in `logs/Transformers` as well. The location of the logs is important to compile metrics and use Results_farm.ipynb


<!-- ## Citing

If you find this repository useful for your work, please consider citing it as follows:

```BibTeX
@inproceedings{Zeng2022AreTE,
  title={Unveiling the Limitations of Transformer Models in Time Series Forecasting},
  author={},
  journal={},
  year={2025}
}
``` -->

Kindly remember to cite all the datasets and compared methods if you use them in your experiments.
